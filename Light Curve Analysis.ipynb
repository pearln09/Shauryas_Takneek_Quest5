{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a9a1180-5aa7-4cdc-82e9-c6a74abf80ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in c:\\users\\pearl narang\\anaconda3\\lib\\site-packages (1.6.17)\n",
      "Requirement already satisfied: tsfresh in c:\\users\\pearl narang\\anaconda3\\lib\\site-packages (0.20.3)\n",
      "Requirement already satisfied: xgboost in c:\\users\\pearl narang\\anaconda3\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\pearl narang\\anaconda3\\lib\\site-packages (0.12.3)\n",
      "Requirement already satisfied: shap in c:\\users\\pearl narang\\anaconda3\\lib\\site-packages (0.46.0)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\pearl narang\\anaconda3\\lib\\site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in c:\\users\\pearl narang\\anaconda3\\lib\\site-packages (from kaggle) (2024.7.4)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\pearl narang\\anaconda3\\lib\\site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: requests in c:\\users\\pearl narang\\anaconda3\\lib\\site-packages (from kaggle) (2.32.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pearl narang\\anaconda3\\lib\\site-packages (from kaggle) (4.66.4)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\pearl narang\\anaconda3\\lib\\site-packages (from kaggle) (5.0.2)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\pearl narang\\anaconda3\\lib\\site-packages (from kaggle) (2.2.2)\n",
      "Requirement already satisfied: bleach in c:\\users\\pearl narang\\anaconda3\\lib\\site-packages (from kaggle) (4.1.0)\n",
      "Requirement already satisfied: numpy>=1.15.1 in c:\\users\\pearl narang\\anaconda3\\lib\\site-packages (from tsfresh) (1.26.4)\n",
      "Requirement already satisfied: pandas>=0.25.0 in c:\\users\\pearl narang\\anaconda3\\lib\\site-packages (from tsfresh) (2.2.2)\n",
      "Requirement already satisfied: statsmodels>=0.13 in c:\\users\\pearl narang\\anaconda3\\lib\\site-packages (from tsfresh) (0.14.2)\n",
      "Requirement already satisfied: patsy>=0.4.1 in c:\\users\\pearl narang\\anaconda3\\lib\\site-packages (from tsfresh) (0.5.6)\n",
      "Requirement already satisfied: scikit-learn>=0.22.0 in c:\\users\\pearl narang\\anaconda3\\lib\\site-packages (from tsfresh) (1.4.2)\n",
      "Requirement already satisfied: stumpy>=1.7.2 in c:\\users\\pearl narang\\anaconda3\\lib\\site-packages (from tsfresh) (1.13.0)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\pearl narang\\anaconda3\\lib\\site-packages (from tsfresh) (2.2.1)\n",
      "Requirement already satisfied: scipy>=1.14.0 in c:\\users\\pearl narang\\anaconda3\\lib\\site-packages (from tsfresh) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\pearl narang\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\pearl narang\\anaconda3\\lib\\site-packages (from imbalanced-learn) (2.2.0)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\users\\pearl narang\\anaconda3\\lib\\site-packages (from shap) (23.2)\n",
      "Requirement already satisfied: slicer==0.0.8 in c:\\users\\pearl narang\\anaconda3\\lib\\site-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba in c:\\users\\pearl narang\\anaconda3\\lib\\site-packages (from shap) (0.59.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pearl narang\\anaconda3\\lib\\site-packages (from pandas>=0.25.0->tsfresh) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\pearl narang\\anaconda3\\lib\\site-packages (from pandas>=0.25.0->tsfresh) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pearl narang\\anaconda3\\lib\\site-packages (from requests->kaggle) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pearl narang\\anaconda3\\lib\\site-packages (from requests->kaggle) (3.7)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in c:\\users\\pearl narang\\anaconda3\\lib\\site-packages (from numba->shap) (0.42.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pearl narang\\anaconda3\\lib\\site-packages (from tqdm->kaggle) (0.4.6)\n",
      "Requirement already satisfied: webencodings in c:\\users\\pearl narang\\anaconda3\\lib\\site-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\pearl narang\\anaconda3\\lib\\site-packages (from python-slugify->kaggle) (1.3)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Setup and Download Dataset\n",
    "\n",
    "# Install necessary packages\n",
    "!pip install kaggle tsfresh xgboost imbalanced-learn shap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "455d76f4-5b44-4867-a771-c88f2f42c8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1e2a0fc-ced3-435f-9d48-5f65cb1c5b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle API credentials\n",
    "#api_token = {\"username\":\"pearlnarang\",\"key\":\"91d31420bce9fea48964d3e3bfb2b028\"}\n",
    "\n",
    "# Create Kaggle directory if it doesn't exist\n",
    "#+os.makedirs(os.path.expanduser(\"~/.kaggle\"), exist_ok=True)\n",
    "\n",
    "# Save API token to kaggle.json\n",
    "#with open(os.path.expanduser(\"~/.kaggle/kaggle.json\"), \"w\") as file:\n",
    " #   json.dump(api_token, file)\n",
    "\n",
    "# Set permissions for the kaggle.json file\n",
    "#os.chmod(os.path.expanduser(\"~/.kaggle/kaggle.json\"), 0o600)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d9ac378-1407-4de6-b2c7-d4a7dd50779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#import pandas as pd\n",
    "\n",
    "# Set the directory containing your files\n",
    "#directory = 'Desktop/My Practice/PLAsTiCC-2018.zip'\n",
    "\n",
    "# Loop through all files in the directory\n",
    "#for filename in os.listdir(directory):\n",
    " #   if filename.endswith(\".csv\"):  # Check if the file is a .csv file\n",
    "  #      file_path = os.path.join(directory, filename)  # Full path to the file\n",
    "        \n",
    "        # Read the CSV file into a DataFrame\n",
    "   #     df = pd.read_csv(file_path)\n",
    "        \n",
    "    #    print(f\"Loaded: {filename}\")  # Optional: Print which file was loaded\n",
    "        # Do something with the DataFrame, e.g., store it, process it, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebe7fb41-480e-43f0-91c3-c10400aec59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the PLAsTiCC dataset\n",
    "#!kaggle competitions download -c PLAsTiCC-2018\n",
    "#!unzip -o '*.zip' -d ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d16f408c-1a92-4a09-8e7f-3058e3514d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load and Preprocess the Dataset\n",
    "\n",
    "# Load the dataset\n",
    "train_data = pd.read_csv('training_set.csv')\n",
    "train_metadata = pd.read_csv('training_set_metadata.csv')\n",
    "\n",
    "# Merge light curves with their metadata\n",
    "merged_data = pd.merge(train_data, train_metadata, on='object_id')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4528ebb7-fac6-46e1-9f19-f4c618226f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 40/40 [16:27<00:00, 24.70s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of extracted_features:\n",
      "   object_id  flux__variance_larger_than_standard_deviation  \\\n",
      "0        615                                            1.0   \n",
      "1        713                                            1.0   \n",
      "2        730                                            1.0   \n",
      "3        745                                            1.0   \n",
      "4       1124                                            1.0   \n",
      "\n",
      "   flux__has_duplicate_max  flux__has_duplicate_min  flux__has_duplicate  \\\n",
      "0                      0.0                      0.0                  0.0   \n",
      "1                      0.0                      0.0                  0.0   \n",
      "2                      0.0                      0.0                  0.0   \n",
      "3                      0.0                      0.0                  0.0   \n",
      "4                      0.0                      0.0                  0.0   \n",
      "\n",
      "   flux__sum_values  flux__abs_energy  flux__mean_abs_change  \\\n",
      "0     -43330.143249      5.985203e+07             202.114067   \n",
      "1       -498.172760      1.532369e+04               2.935177   \n",
      "2        748.253237      2.286984e+04               4.227614   \n",
      "3       3127.131254      2.936695e+05               7.065548   \n",
      "4       2515.287161      1.591006e+05               6.727352   \n",
      "\n",
      "   flux__mean_change  flux__mean_second_derivative_central  ...  \\\n",
      "0           1.999688                              0.276020  ...   \n",
      "1          -0.050944                              0.007147  ...   \n",
      "2          -0.008131                             -0.000364  ...   \n",
      "3           0.008044                              0.008321  ...   \n",
      "4           0.012543                              0.007607  ...   \n",
      "\n",
      "   flux__fourier_entropy__bins_5  flux__fourier_entropy__bins_10  \\\n",
      "0                       0.530599                        0.871525   \n",
      "1                       0.125256                        0.204871   \n",
      "2                       0.125256                        0.215617   \n",
      "3                       0.421225                        0.731048   \n",
      "4                       0.079983                        0.090729   \n",
      "\n",
      "   flux__fourier_entropy__bins_100  \\\n",
      "0                         2.447764   \n",
      "1                         1.636941   \n",
      "2                         1.947293   \n",
      "3                         2.258537   \n",
      "4                         0.526015   \n",
      "\n",
      "   flux__permutation_entropy__dimension_3__tau_1  \\\n",
      "0                                       1.590345   \n",
      "1                                       1.784294   \n",
      "2                                       1.777388   \n",
      "3                                       1.785259   \n",
      "4                                       1.784306   \n",
      "\n",
      "   flux__permutation_entropy__dimension_4__tau_1  \\\n",
      "0                                       2.635893   \n",
      "1                                       3.126691   \n",
      "2                                       3.131224   \n",
      "3                                       3.139805   \n",
      "4                                       3.148199   \n",
      "\n",
      "   flux__permutation_entropy__dimension_5__tau_1  \\\n",
      "0                                       3.694406   \n",
      "1                                       4.557051   \n",
      "2                                       4.534929   \n",
      "3                                       4.558918   \n",
      "4                                       4.612072   \n",
      "\n",
      "   flux__permutation_entropy__dimension_6__tau_1  \\\n",
      "0                                       4.574263   \n",
      "1                                       5.534593   \n",
      "2                                       5.447385   \n",
      "3                                       5.513080   \n",
      "4                                       5.507628   \n",
      "\n",
      "   flux__permutation_entropy__dimension_7__tau_1  \\\n",
      "0                                       5.200040   \n",
      "1                                       5.792283   \n",
      "2                                       5.713333   \n",
      "3                                       5.803362   \n",
      "4                                       5.746273   \n",
      "\n",
      "   flux__query_similarity_count__query_None__threshold_0.0  \\\n",
      "0                                                NaN         \n",
      "1                                                NaN         \n",
      "2                                                NaN         \n",
      "3                                                NaN         \n",
      "4                                                NaN         \n",
      "\n",
      "   flux__mean_n_absolute_max__number_of_maxima_7  \n",
      "0                                    1088.398280  \n",
      "1                                      13.962413  \n",
      "2                                      41.174077  \n",
      "3                                     166.468631  \n",
      "4                                     120.439986  \n",
      "\n",
      "[5 rows x 784 columns]\n",
      "Columns in extracted_features after adding object_id:\n",
      "Index(['object_id', 'flux__variance_larger_than_standard_deviation',\n",
      "       'flux__has_duplicate_max', 'flux__has_duplicate_min',\n",
      "       'flux__has_duplicate', 'flux__sum_values', 'flux__abs_energy',\n",
      "       'flux__mean_abs_change', 'flux__mean_change',\n",
      "       'flux__mean_second_derivative_central',\n",
      "       ...\n",
      "       'flux__fourier_entropy__bins_5', 'flux__fourier_entropy__bins_10',\n",
      "       'flux__fourier_entropy__bins_100',\n",
      "       'flux__permutation_entropy__dimension_3__tau_1',\n",
      "       'flux__permutation_entropy__dimension_4__tau_1',\n",
      "       'flux__permutation_entropy__dimension_5__tau_1',\n",
      "       'flux__permutation_entropy__dimension_6__tau_1',\n",
      "       'flux__permutation_entropy__dimension_7__tau_1',\n",
      "       'flux__query_similarity_count__query_None__threshold_0.0',\n",
      "       'flux__mean_n_absolute_max__number_of_maxima_7'],\n",
      "      dtype='object', length=784)\n",
      "Columns in train_metadata before merge:\n",
      "Index(['object_id', 'ra', 'decl', 'gal_l', 'gal_b', 'ddf', 'hostgal_specz',\n",
      "       'hostgal_photoz', 'hostgal_photoz_err', 'distmod', 'mwebv', 'target'],\n",
      "      dtype='object')\n",
      "Merge successful. Columns in features:\n",
      "Index(['object_id', 'flux__variance_larger_than_standard_deviation',\n",
      "       'flux__has_duplicate_max', 'flux__has_duplicate_min',\n",
      "       'flux__has_duplicate', 'flux__sum_values', 'flux__abs_energy',\n",
      "       'flux__mean_abs_change', 'flux__mean_change',\n",
      "       'flux__mean_second_derivative_central',\n",
      "       ...\n",
      "       'flux__fourier_entropy__bins_10', 'flux__fourier_entropy__bins_100',\n",
      "       'flux__permutation_entropy__dimension_3__tau_1',\n",
      "       'flux__permutation_entropy__dimension_4__tau_1',\n",
      "       'flux__permutation_entropy__dimension_5__tau_1',\n",
      "       'flux__permutation_entropy__dimension_6__tau_1',\n",
      "       'flux__permutation_entropy__dimension_7__tau_1',\n",
      "       'flux__query_similarity_count__query_None__threshold_0.0',\n",
      "       'flux__mean_n_absolute_max__number_of_maxima_7', 'target'],\n",
      "      dtype='object', length=785)\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering using tsfresh\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.feature_extraction import ComprehensiveFCParameters\n",
    "\n",
    "# Preserve the 'object_id' before extraction\n",
    "object_ids = merged_data[['object_id']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Extract features automatically\n",
    "extracted_features = extract_features(merged_data, column_id=\"object_id\", column_sort=\"mjd\", \n",
    "                                      column_value=\"flux\", default_fc_parameters=ComprehensiveFCParameters())\n",
    "\n",
    "# Reset index and manually add 'object_id'\n",
    "extracted_features.reset_index(drop=True, inplace=True)\n",
    "extracted_features = pd.concat([object_ids, extracted_features], axis=1)\n",
    "\n",
    "# Ensure that 'object_id' is now included\n",
    "print(\"First few rows of extracted_features:\")\n",
    "print(extracted_features.head())\n",
    "print(\"Columns in extracted_features after adding object_id:\")\n",
    "print(extracted_features.columns)\n",
    "\n",
    "# Ensure 'object_id' is present in both DataFrames before merging\n",
    "print(\"Columns in train_metadata before merge:\")\n",
    "print(train_metadata.columns)\n",
    "\n",
    "# Merge with metadata for labels\n",
    "try:\n",
    "    features = pd.merge(extracted_features, train_metadata[['object_id', 'target']], on='object_id')\n",
    "    print(\"Merge successful. Columns in features:\")\n",
    "    print(features.columns)\n",
    "except KeyError as e:\n",
    "    print(f\"KeyError: {e}. Check if 'object_id' exists in both DataFrames.\")\n",
    "    features = None  # Set features to None to avoid proceeding with errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c1231a8-2d26-4b35-aea0-106406efdd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Model Selection using XGBoost\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4945ee22-f1d7-4d63-997d-e843437754ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes after encoding: [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
      "Encoded classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "Accuracy: 0.6898089171974522\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           6       0.75      0.56      0.64        32\n",
      "          15       0.54      0.29      0.38        97\n",
      "          16       0.97      0.96      0.97       198\n",
      "          42       0.48      0.42      0.45       222\n",
      "          52       0.00      0.00      0.00        46\n",
      "          53       1.00      0.91      0.95        11\n",
      "          62       0.64      0.24      0.34        89\n",
      "          64       0.50      0.22      0.31        18\n",
      "          65       0.89      0.95      0.92       195\n",
      "          67       0.50      0.03      0.06        32\n",
      "          88       0.97      0.92      0.94        73\n",
      "          90       0.57      0.87      0.69       471\n",
      "          92       0.98      1.00      0.99        43\n",
      "          95       0.91      0.23      0.37        43\n",
      "\n",
      "    accuracy                           0.69      1570\n",
      "   macro avg       0.69      0.54      0.57      1570\n",
      "weighted avg       0.68      0.69      0.66      1570\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Proceed only if merge was successful\n",
    "if features is not None:\n",
    "   # Split data into training and testing sets\n",
    "    X = features.drop(['target', 'object_id'], axis=1)\n",
    "    y = features['target']\n",
    "\n",
    "    # Create a mapping from original labels to consecutive integers\n",
    "    unique_labels = np.unique(y)\n",
    "    label_mapping = {label: idx for idx, label in enumerate(sorted(unique_labels))}\n",
    "    \n",
    "    # Apply the mapping to encode the labels\n",
    "    y_encoded = y.map(label_mapping)\n",
    "\n",
    "    # Verify the classes after encoding\n",
    "    print(f\"Classes after encoding: {sorted(label_mapping.keys())}\")\n",
    "    print(f\"Encoded classes: {sorted(label_mapping.values())}\")\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize XGBoost model\n",
    "    xgb_model = xgb.XGBClassifier(objective=\"multi:softprob\", eval_metric=\"mlogloss\")\n",
    "\n",
    "    # Train the model\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "    # Decode the predictions back to original labels\n",
    "    reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "    y_pred_decoded = np.array([reverse_label_mapping[p] for p in y_pred])\n",
    "    y_test_decoded = np.array([reverse_label_mapping[t] for t in y_test])\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(f\"Accuracy: {accuracy_score(y_test_decoded, y_pred_decoded)}\")\n",
    "    print(classification_report(y_test_decoded, y_pred_decoded, zero_division=0))\n",
    "\n",
    "else:\n",
    "    print(\"Skipping model training due to previous errors.\")\n",
    "\n",
    "# Step 4: Hyperparameter Tuning using GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e550a88-5eb7-4143-bfcd-5a43e9900fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ee8bcd-9968-48fe-b64d-60b2872df29a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4c479a-cc51-4a76-92fe-04368d8a7013",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac215625-95a7-4a9d-b677-8db5d2d4e3e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d8e8d3-1f13-47d8-aad9-0f4b037c3485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b47abd-781c-4553-9de3-fa17e30dc8c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
